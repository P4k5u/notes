### 简介
分布式 ID 用来解决分布式系统中数据库分库分表的情况下，不能保证主键唯一性的问题
分布式 ID 方案，一般分为两种：
- 类 DB
- 类 Snowflake
类 DB 型的可以设置不同起始值和步长来实现趋势递增，需要考虑服务的容错性和可用性
类 snowflake 就是将 64 位划分为不同的段，每段代表不同的涵义，基本就是时间戳、机器ID 和 序列数，这种方案需要考虑时钟回拨问题以及一些 buffer 的缓冲设计

### UUID
UUID（Universally Unique Identifier），通用唯一识别码的缩写。UUID 是由一组 32 位数的 16 进制数字构成
生存的 UUID 是由 8-4-4-4-12 格式的数据组成，其中 32 个字符和 4 个连字符 ’-‘ ,一般使用的时候会用把 '-' 删除

UUID 的生成方式有五种：
- 基于时间的 UUID
- DCE 安全的 UUID
- 基于名字的 UUID（MD5）
- 随机 UUID
- 基于名字的 UUID （SHA1）

UUID 可以在本地生成，没有网络消耗，但是也会有一些缺点：
- 不易与存储
- 信息不安全
- 对 MySQL 索引不利
### 数据库生成
因为分布式数据库的起始自增值一样所以才会有冲突的情况发生，将分布式数据库中的同一个业务表的自增 ID 设计成不一样的起始值，然后设置固定的步长，步长的值即为分库的数量或分表的数量

以 MySQL 为例，给字段设置 `auto_increment_increment` 和 `auto_increment_offset` 来保证 ID 自增
- auto_increment_offset：表示自增字段的起始数值
- auto_increment_increment：表示步长

假设有三台机器，则DB1中order表的起始ID值为1，DB2中order表的起始值为2，DB3中order表的起始值为3，它们自增的步长都为3，则它们的ID生成范围如下图所示：![[Pasted image 20240103173406.png]]

通过这种方式明显的优势就是依赖于数据库自身不需要其他资源，并且ID号单调自增，可以实现一些对ID有特殊要求的业务。

但是缺点也很明显，首先它**强依赖DB**，当DB异常时整个系统不可用。虽然配置主从复制可以尽可能的增加可用性，但是**数据一致性在特殊情况下难以保证**。主从切换时的不一致可能会导致重复发号。还有就是**ID发号性能瓶颈限制在单台MySQL的读写性能**

### Redis 实现
Redis实现分布式唯一ID主要是通过提供像 `INCR` 和 `INCRBY` 这样的自增原子命令，由于Redis自身的单线程的特点所以能保证生成的 ID 肯定是唯一有序的

但是单机存在性能瓶颈，无法满足高并发的业务需求，所以可以采用集群的方式来实现。集群的方式又会涉及到和数据库集群同样的问题，所以也需要设置分段和步长来实现

### 雪花算法 Snowflake
Snowflake，雪花算法是由Twitter开源的分布式ID生成算法，以划分命名空间的方式将 64-bit 分割成多个部分，每个部分代表不同的含义。而 Java 中64bit的整数是 Long 类型，所以在 Java 中 SnowFlake 算法生成的 ID 就是 long 来存储的。
- **第1位**占用 1bit，其值始终是0，可看做是符号位不使用。
- **第2位**开始的 41bit 是时间戳，41-bit位可表示2^41个数，每个数代表毫秒，那么雪花算法可用的时间年限是`(1L<<41)/(1000L360024*365)`= 69 年的时间。
- **中间的 10bit 位** 可表示机器数，即2^10 = 1024台机器，但是一般情况下我们不会部署这么台机器。如果我们对IDC（互联网数据中心）有需求，还可以将 10-bit 分 5-bit 给 IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，具体的划分可以根据自身需求定义。
- **最后12bit位** 是自增序列，可表示2^12 = 4096个数

这样的划分之后相当于**在一毫秒一个数据中心的一台机器上可产生4096个有序的不重复的ID**。但是我们 IDC 和机器数肯定不止一个，所以毫秒内能生成的有序ID数是翻倍的![[Pasted image 20240103174011.png]]

**雪花算法提供了一个很好的设计思想，雪花算法生成的ID是趋势递增，不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的，而且可以根据自身业务特性分配bit位，非常灵活**。

但是雪花算法强**依赖机器时钟**，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。如果恰巧回退前生成过一些ID，而时间回退后，生成的ID就有可能重复。官方对于此并没有给出解决方案，而是简单的抛错处理，这样会造成在时间被追回之前的这段时间服务不可用

#### 时钟回拨
解决时钟回拨的方法，要注意期间保持服务的可用：
- 记录发生每个产生id的时间，在发生时钟回拨时，替换为手动的自增操作，但是这样违背了snowflake 跟时间绑定的特性，如果业务id需要跟时间要求一致的话，就失去了业务含义
### Mist 薄雾算法

### 开源框架

#### 美团 Leaf
Leaf 提供了两种 ID 生成的方式，分别是 Leaf-segment 数据库方案 和 Leaf-snowflake 方案

##### Leaf-segment 方案
这是基于数据库的方案，但做了以下改动：
- 原来每次获取ID都要读写一次数据库，造成数据库压力过大，现在改为利用 proxy server 批量获取，每次获取一个 segment 号段的值，用完再去数据库获取新的号段
- 各个业务不同的发号需求用 biz_tag 字段来区分，每个 biz_tag 的 ID 获取互相隔离
数据库设计如下：
```sql
CREATE TABLE `leaf_alloc` ( `biz_tag` varchar(128) NOT NULL DEFAULT '' COMMENT '业务key', `max_id` bigint(20) NOT NULL DEFAULT '1' COMMENT '当前已经分配了的最大id', `step` int(11) NOT NULL COMMENT '初始步长，也是动态调整的最小步长', `description` varchar(256) DEFAULT NULL COMMENT '业务key的描述', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间', PRIMARY KEY (`biz_tag`) ) ENGINE=InnoDB;
```
原来每次获取ID都要写数据库，现在只需要把 step 设置得足够大，比如：1000。那么就可大大减少获取 ID 的频率
同时也提供了双buffer机制进行优化，防止取号时阻塞，在号段消耗到一定程度的时候就进行取号，防止在取号发生阻塞导致没有可用的 ID![[Pasted image 20240108162152.png]]
##### Leaf-snowflake 方案
Leaf-snowflake 方案完全沿用 snowflake 方案的 bit 位设计，对于 workerID 的分配引入了 zookeeper 持久顺序节点的特性，自动对 snowflake 节点配置 workerID

Leaf-snowflake 的启动步骤如下：
- 启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）
- 如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务
- 如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务![[Pasted image 20240108163025.png]]
为了减少对 Zookeeper的依赖性，会在本机文件系统上缓存一个workerID文件。当ZooKeeper出现问题，恰好机器出现问题需要重启时，能保证服务能够正常启动

在类 snowflake算法上都存在时钟回拨的问题，Leaf-snowflake在解决时钟回拨的问题上是通过校验自身系统时间与 `leaf_forever/${self}`节点记录时间做比较然后启动报警的措施![[Pasted image 20240108163125.png]]
#### 滴滴 TinyId